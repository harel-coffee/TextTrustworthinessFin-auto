# -*- coding: utf-8 -*-
"""
Created on Mon Oct 16 11:05:37 2017

@author: JanneK

Pipeline for fake text project
"""

import pickle
import os
import numpy as np
from sklearn.metrics import mean_squared_error
from FeatureExtractor import FeatureExtractor
from sklearn.metrics import f1_score
from sklearn.preprocessing import MinMaxScaler
from TextFeatures import TextFeatures
import Utils
import pandas
from functools import partial
from multiprocessing import Pool
import copy
from scipy.stats import spearmanr

class MainPipeline(object):

    def __init__(self,
                 RootPath='',
                 ResultPath = None,
                 RECOMPUTE_ALL=0,
                 RANDOM_TEST=0,
                 SINGLE_THREAD_TEST=0,
                 CV_folds = None,
                 Type = 'regression',
                 LearnParams=None,
                 run_type='simple',
                 TargetSource='BASELINE_ALS',
                 N_workers = 8,
                 dimensions=None,
                 is_sequential=False,
                 ensemble_mix=(1,),
                 verbose_level = 1, # 0 = nothing, 1 = final result, 2 = all results
                 ):
        self.RECOMPUTE_ALL = RECOMPUTE_ALL
        self.RANDOM_TEST = RANDOM_TEST
        self.RootPath = RootPath
        self.ResultPath = ResultPath

        self.CV_folds=CV_folds
        self.Type=Type
        self.is_sequential=is_sequential

        self.dimensions = dimensions
        # set dimensions to analyze
        if isinstance(self.dimensions,tuple) or isinstance(self.dimensions,list):
            pass
        elif self.dimensions is None or self.dimensions=='all':
            self.dimensions = ('reliability','sentiment','writestyle','textlogic','infovalue','subjectivity')
        elif isinstance(self.dimensions,str):
            self.dimensions=(self.dimensions,)
        else:
            raise(Exception('Error in setting up dimensions to analyze!'))

        self.run_type=run_type
        assert run_type in ['simple','looper','final','looper_local']
        self.TargetSource=TargetSource
        self.N_workers=np.minimum(N_workers,self.CV_folds)
        self.verbose_level = verbose_level
        self.SINGLE_THREAD_TEST=SINGLE_THREAD_TEST
        self.PREPROCESSED_X=None
        self.PREPROCESSED_Y=None
        self.RAW_X = None
        self.METADATA=None
        self.STRINGKERNELS = None

        self.Params = {}

        # /media/jannek/Data/JanneK/Documents/git_repos/LaureaTextAnalyzer/
        self.Params['RootPath'] = self.RootPath

        # main test/train split degree
        self.Params['CV-folds'] = self.CV_folds
        self.Params['TargetType'] = self.Type #'['classification',5]
        self.Params['TextFeatures_object']=None
        self.Params['TargetSource'] = self.TargetSource

        # make sure LearnParams is list, where each element contains parameters for one model
        # this is to allow ensemble models
        #if isinstance(LearnParams,dict):
        #    self.LearnParams = [LearnParams] # all parameters related to learning (data, method, method params, etc.)
        self.LearnParams = LearnParams
        if isinstance(LearnParams,list) or isinstance(LearnParams,tuple):
            self.LearnParams = {x:LearnParams for x in self.dimensions}
        assert len(self.LearnParams)>0

    def run(self):

        #Params = self.Params

        try:
            # file that contains POS tagged texts (generated by TurkuNLP pipeline)
            self.Params['INPUT-texts'] = self.Params['RootPath'] + r'data/texts/parsed_texts_combined_TAGGED_analyzed.txt'
            # file that contains metainfo (generated at the parsing stage)
            self.Params['INPUT-metainfo'] = self.Params['RootPath'] + r'data/texts/parsed_metainfo.txt'
            # file that contains target information (generated by Surprise code)
            self.Params['INPUT-targets'] = self.Params['RootPath'] + r'data/targets/' + r'SURPRISE_RESULTS.pickle'
            # folder to dump all results
            if self.ResultPath is None:
                self.Params['OUTPUT'] = self.Params['RootPath'] + r'results/'
            else:
                self.Params['OUTPUT'] = self.ResultPath

            assert(os.path.isfile(self.Params['INPUT-metainfo']))
            assert(os.path.isfile(self.Params['INPUT-texts']))
            assert(os.path.isfile(self.Params['INPUT-targets']))
            assert(os.path.isdir(self.Params['OUTPUT']))

        except:
            raise('Initial data path setup check failed!')

        # create external data object
        self.Params['ExternalData'] = TextFeatures(ROOT_PATH=self.Params['RootPath'] + r'data/external/',RECOMPUTE_ALL=self.RECOMPUTE_ALL)
        # load lists that do not need actual task data
        self.Params['ExternalData'].load_external_lists()

        #%% Start pipeline

        # PART 1. PREPROCESSING, ALL DATA, can only consider individual datapoints!
        # OUTPUT: processed documents (words) and non-word custom features
        datafile = self.Params['RootPath'] + r'results/' + 'PREPROCESSED_data.pickle'
        if not os.path.isfile(datafile) or self.RECOMPUTE_ALL:
            import Preprocessor
            # get data
            self.PREPROCESSED_X,self.RAW_X,self.METADATA,self.PREPROCESSED_Y,self.STRINGKERNELS = Preprocessor.main(self.Params)
            # load embedding data for our corpus (save memory and time)
            self.Params['ExternalData'].load_external_embeddings(self.PREPROCESSED_X)
            # compute custom features (counts)
            self.CUSTOMDATA = self.Params['ExternalData'].get_length_features(self.PREPROCESSED_X)
            # add ratios
            self.METADATA = self.Params['ExternalData'].add_ratios(self.METADATA)
            # dump as pickle
            pickle.dump((self.PREPROCESSED_X,self.RAW_X,self.METADATA,self.CUSTOMDATA,self.PREPROCESSED_Y,self.STRINGKERNELS,self.Params), open(datafile, "wb" ))
        else:
            # load pickle
            self.PREPROCESSED_X,self.RAW_X,self.METADATA,self.CUSTOMDATA,self.PREPROCESSED_Y,self.STRINGKERNELS,Params_loaded = pickle.load(open( datafile, "rb" ))
            # load embeddings
            self.Params['ExternalData'].load_external_embeddings(self.PREPROCESSED_X)

        # result columns
        self.columns = ['F1','F1_null','R','spearmanR','R2','MSE_null','MSE','REAL_Y','PREDICTED_Y','NULL_Y','TEXT_IDs','FEATURES','MODEL','FOLD','FEATURE_SCALE_MULTIPLIER','X_DATA']

        # set up binning object if not doing regression
        if self.Params['TargetType']!='regression':
            if self.Params['TargetType'][0]=='classification' and 2<self.Params['TargetType'][1]<20:
                self.PREPROCESSED_Y,class_ids = Utils.do_binning(self.PREPROCESSED_Y,self.Params['TargetType'][1])
            else:
                raise(Exception('Bad classification setup'))

        # create stratified k-folds, same for all dimensions
        self.my_kfold = Utils.Kfold(self.PREPROCESSED_Y,self.Params['TargetType'],n_splits=self.Params['CV-folds'], random_state=666)

        # compute folds
        if self.SINGLE_THREAD_TEST==1:
            print('!!! Using single-thread mode !!!')
            RESULTS_pool=[]
            for fold_k in range(0, self.Params['CV-folds']):
                RESULTS_pool.append(self.run_fold(fold_k))
        else:
            pool = Pool(processes=self.N_workers)
            temp = list(range(0,self.Params['CV-folds']))
            RESULTS_pool = pool.map(partial(self.run_fold),temp)
            pool.close()
            pool.join()

        # compute with full data, if requested
        if self.run_type=='final':
            print(' Fitting with full data ')
            self.my_kfold = Utils.Kfold(self.PREPROCESSED_Y, self.Params['TargetType'], n_splits=1, random_state=666)
            RESULTS_final = self.run_fold(0,total_folds = 1)

        # nullify some data to save memory
        self.Params['ExternalData']=None

        RESULTS = {}
        rows = list(range(1,self.CV_folds+1)) + ['total']
        for dimension in self.dimensions:
            RESULTS[dimension]={'train':pandas.DataFrame(index=rows,columns=self.columns),'test':pandas.DataFrame(index=rows,columns=self.columns),'params':'NOT SET!'}

        for dimension in self.dimensions:
            for typ in ['train','test']:
                for column in self.columns:
                    for fold_k in range(0, len(RESULTS_pool)):
                        RESULTS[dimension][typ].loc[fold_k+1,column] = RESULTS_pool[fold_k][dimension][typ][column]
            RESULTS[dimension]['params'] ='folds='+str(self.CV_folds) + ', ' + str(self.LearnParams)

        assert self.Params['CV-folds'] == len(RESULTS_pool) == self.CV_folds, 'Fold count is incorrect!'

        # compute pooled results
        RESULTS = Utils.pool_results(RESULTS,folds=self.Params['CV-folds'])

        if self.verbose_level > 0:
            print('\n--------------- FINAL RESULTS over all %i folds---------------' % self.Params['CV-folds'])
            Utils.print_results(RESULTS)
            print('--------------------------------------------------------------\n')

        for dimension in self.dimensions:
            if self.run_type == 'final':
                RESULTS[dimension]['full_data'] = RESULTS_final[dimension]['train']
            if self.run_type == 'looper':
                RESULTS[dimension] = Utils.reduce_results(RESULTS[dimension])

        return RESULTS


    def get_sequential_predictors(self,Y_predicted_ensemble_previous,dimension,ensemble_loop):
        dims = list(Y_predicted_ensemble_previous.keys())
        Y=[]
        for dim in dims:
            if dim!=dimension:
                Y.append(Y_predicted_ensemble_previous[dim][ensemble_loop])
        Y=np.vstack(Y).transpose()
        return Y

    def run_fold(self,fold_k,total_folds = -1):

        # set total folds
        if total_folds==-1:
            total_folds = self.Params['CV-folds']

        if self.verbose_level>1:
            print('\n------- STARTING FOLD %i of %i ---------\n' % (fold_k + 1,total_folds))

        # initialize columns for results
        RESULTS = {dimension:{'train':{col:np.nan for col in self.columns},'test':{col:np.nan for col in self.columns}} for dimension in self.dimensions}

        # keep this to speed up computation over dimensions, only computed once
        # remains unchanged for all dimensions
        cached_embedded_transformer = None

        ALL_dimensions = ('reliability','sentiment','writestyle','textlogic','infovalue','subjectivity')

        sequential_loops=(0,)
        if self.is_sequential:
            sequential_loops = (0,1)

        Y_train_predicted_ensemble_previous = {}
        Y_test_predicted_ensemble_previous = {}
        for seq_loop in sequential_loops:
            for dim_loop,dimension in enumerate(self.dimensions):

                other_dimensions = set(ALL_dimensions)
                other_dimensions.discard(dimension)

                assert len(other_dimensions)+1 == len(ALL_dimensions)

                # get train and test IDs
                train_IDs = self.my_kfold[0][dimension][fold_k]
                test_IDs = self.my_kfold[1][dimension][fold_k]

                assert len(set(train_IDs) & set(test_IDs))==0,'Train and test sets overlapping!'

                # split to test and train (does not modify any data!)
                X_DATA_train = Utils.extract_dict_data(self.PREPROCESSED_X, train_IDs)
                X_DATA_test = Utils.extract_dict_data(self.PREPROCESSED_X, test_IDs)

                X_METADATA_train = [self.METADATA[1][x] for x in train_IDs]
                X_METADATA_test = [self.METADATA[1][x] for x in test_IDs]

                X_CUSTOMDATA_train = [self.CUSTOMDATA[1][x] for x in train_IDs]
                X_CUSTOMDATA_test = [self.CUSTOMDATA[1][x] for x in test_IDs]

                Y_train = np.array([self.PREPROCESSED_Y[dimension][x] for x in train_IDs])
                Y_test = np.array([self.PREPROCESSED_Y[dimension][x] for x in test_IDs])

                Y_predicted_other = []
                Y_train_other = []
                for other_d in other_dimensions:
                    Y_train_other.append(np.expand_dims(np.array([self.PREPROCESSED_Y[other_d][x] for x in train_IDs]),axis=1))
                    Y_predicted_other.append(np.expand_dims(np.array([self.PREPROCESSED_Y[other_d][x] for x in train_IDs]),axis=1))
                Y_train_other = np.concatenate(tuple(Y_train_other),axis=1)

                # shuffle targets id doing sanity checking, should result random model
                if self.RANDOM_TEST:
                    print('!!! WARNING! Randomizing input and creating fully a NULL MODEL !!!')
                    np.random.shuffle(Y_train)
                    np.random.shuffle(Y_test)

                # create lists for ensemble results
                Y_train_predicted_ensemble=[]
                Y_predicted_ensemble=[]
                X_columns_ensemble=[]
                X_train_ensemble = []
                MODEL_predictor_ensemble=[]

                # loop over all learning algorithms (one or two)
                for ensemble_loop,raw_LearnParams in enumerate(self.LearnParams[dimension]):

                    # check and convert parameters, make a copy to avoid modifying originals
                    local_LearnParams = Utils.process_params(copy.deepcopy(raw_LearnParams))

                    if 'pass1_features_SVD' not in local_LearnParams:
                        local_LearnParams['pass1_features_SVD']=None

                    feat_object = FeatureExtractor(embedded_transformer = cached_embedded_transformer,external_features=self.Params['ExternalData'],FEATURE_SCALER = local_LearnParams['FeatureScaler'])

                    X_train,X_columns,local_LearnParams = feat_object.main(X_DATA_train, Y_train,local_LearnParams,x_meta=(self.METADATA[0],X_METADATA_train),x_custom=(self.CUSTOMDATA[0],X_CUSTOMDATA_train),print_info = self.verbose_level>1,text_IDs=(train_IDs,),stringkernels = self.STRINGKERNELS)

                    if dim_loop==0: # embedding weights only depend on X and are the same between all dimensions
                        cached_embedded_transformer = feat_object.embedded_transformer

                    if seq_loop==1:
                        X_addition = self.get_sequential_predictors(Y_train_predicted_ensemble_previous,dimension,ensemble_loop)
                        X_train = np.concatenate((X_train,X_addition),axis=1)
                        X_columns = X_columns+list(other_dimensions)

                    # PART 3: TRAIN REGRESSION MODEL
                    MODEL_predictor=None
                    post_transformer=None
                    y_scaler=None
                    if local_LearnParams['Algorithm'][0] == 'SEQUENCE':

                        import SequenceModels

                        feature_range = (0.001, 0.999)
                        y_scaler = MinMaxScaler(feature_range=feature_range)
                        Y_train_mod = y_scaler.fit_transform(np.expand_dims(Y_train, axis=1))
                        #Y_train = Y_train.flatten()

                        if local_LearnParams['Algorithm'][1]['algorithm']=='simple_CNN':
                            post_transformer = lambda x: x[0]['FLAT'] if len(x) == 1 else [x[0]['FLAT'], x[1],]
                            X_train = post_transformer(X_train)
                            MODEL_predictor = SequenceModels.SimpleCNN(X_train, Y_train_mod, local_LearnParams)
                        elif local_LearnParams['Algorithm'][1]['algorithm']=='simple_LSTM':
                            post_transformer = lambda x: x[0]['FLAT'] if len(x) == 1 else [x[0]['FLAT'], x[1],]
                            X_train = post_transformer(X_train)
                            MODEL_predictor = SequenceModels.SimpleLSTM(X_train, Y_train_mod, local_LearnParams)
                        elif local_LearnParams['Algorithm'][1]['algorithm'] == 'FASTTEXT':
                            post_transformer = lambda x: x#x[0] if len(x) == 1 else [x[0], x[1],]
                            X_train = post_transformer(X_train)
                            MODEL_predictor = SequenceModels.fasttext(X_train, Y_train_mod, local_LearnParams)
                        elif local_LearnParams['Algorithm'][1]['algorithm'] == 'simple_CNN2LSTM':
                            post_transformer = lambda x: [x[0]['FLAT']] if len(x) == 1 else [x[0]['SENTENCES'], x[1],]
                            X_train = post_transformer(X_train)
                            MODEL_predictor = SequenceModels.SimpleCNN2LSTM(X_train, Y_train_mod, local_LearnParams)
                        elif local_LearnParams['Algorithm'][1]['algorithm'] == 'simple_HATT':
                            post_transformer = lambda x: [x[0]['SENTENCES']] if len(x) == 1 else [x[0]['SENTENCES'], x[1],]
                            X_train = post_transformer(X_train)
                            MODEL_predictor = SequenceModels.SimpleHATT(X_train, Y_train_mod,local_LearnParams)
                        else:
                            raise(Exception('Sequence model %s not implemented!' % local_LearnParams[1]['algorithm']))

                    else:

                        import StandardModels
                        MODEL_predictor = StandardModels.main(X=X_train, Y=Y_train, Params=local_LearnParams,print_info=self.verbose_level>1,is_regression=self.Type=='regression',Y_other = Y_train_other)

                    # PART 4: use the trained model and save all results
                    Y_train_predicted = MODEL_predictor.predict(X_train)
                    if y_scaler is not None:
                        Y_train_predicted = y_scaler.inverse_transform(Y_train_predicted)
                        # Y_train_mod = y_scaler.inverse_transform(Y_train_mod)
                    if len(Y_train_predicted.shape)>1:
                        Y_train_predicted = Y_train_predicted[:,0]

                    if self.Type != 'regression':
                        Y_train_predicted = Utils.convert_to_label(Y_train_predicted)

                    Y_train_predicted = Y_train_predicted.flatten()

                    Y_train_predicted_ensemble.append(Y_train_predicted)
                    X_columns_ensemble.append(X_columns)
                    MODEL_predictor_ensemble.append(MODEL_predictor)

                    if total_folds>1:

                        X_train = None

                        # APPLY MODEL TO TEST DATA
                        X_test = feat_object.transform(X_DATA_test, x_meta=X_METADATA_test, x_custom=X_CUSTOMDATA_test, post_transformer=post_transformer,text_IDs=(test_IDs,train_IDs),stringkernels = self.STRINGKERNELS)

                        if seq_loop == 1:
                            X_addition = self.get_sequential_predictors(Y_test_predicted_ensemble_previous, dimension, ensemble_loop)
                            X_test = np.concatenate((X_test, X_addition), axis=1)

                        Y_predicted = MODEL_predictor.predict(X_test)
                        if y_scaler is not None:
                            Y_predicted = y_scaler.inverse_transform(Y_predicted)
                        if len(Y_predicted.shape) > 1:
                            Y_predicted = Y_predicted[:, 0]
                        MODEL_predictor = None

                        if self.Type != 'regression':
                            Y_predicted = Utils.convert_to_label(Y_predicted)

                        Y_predicted_ensemble.append(Y_predicted.flatten())

                        X_train_ensemble.append(None)

                    else:

                        X_train_ensemble.append(X_train)

                N = len(Y_train_predicted_ensemble)

                if N>1:
                    Y_train_predicted = 0
                    Y_predicted = 0
                    X_columns=X_columns_ensemble
                    MODEL_predictor=MODEL_predictor_ensemble
                    for k in range(N):
                        Y_train_predicted += Y_train_predicted_ensemble[k]/N
                        if total_folds > 1:
                            Y_predicted += Y_predicted_ensemble[k]/N
                else:
                    Y_train_predicted = Y_train_predicted_ensemble[0]
                    X_train_ensemble = X_train_ensemble[0]
                    X_columns = X_columns_ensemble[0]
                    if total_folds > 1:
                        Y_predicted = Y_predicted_ensemble[0]
                        MODEL_predictor=MODEL_predictor_ensemble[0]

                # save pooled ensemble result
                Y_train = Y_train.flatten()

                RESULTS[dimension]['train']['TEXT_IDs'] = train_IDs
                RESULTS[dimension]['train']['FOLD'] = (fold_k + 1) * np.ones(len(Y_train))
                RESULTS[dimension]['train']['PREDICTED_Y'] = Y_train_predicted
                RESULTS[dimension]['train']['REAL_Y'] = Y_train
                RESULTS[dimension]['train']['R'] = (np.corrcoef(Y_train_predicted, Y_train))[0, 1]
                RESULTS[dimension]['train']['spearmanR'] = spearmanr(Y_train_predicted, Y_train).correlation
                RESULTS[dimension]['train']['R2'] = RESULTS[dimension]['train']['R'] ** 2
                RESULTS[dimension]['train']['NULL_Y'] = np.mean(Y_train) + 0 * Y_train
                RESULTS[dimension]['train']['MSE_null'] = mean_squared_error(Y_train, RESULTS[dimension]['train']['NULL_Y'])
                RESULTS[dimension]['train']['MSE'] = mean_squared_error(Y_train_predicted, Y_train)
                RESULTS[dimension]['train']['MODEL'] = MODEL_predictor
                RESULTS[dimension]['train']['FEATURE_SCALE_MULTIPLIER'] = feat_object.feature_scale_multiplier
                RESULTS[dimension]['train']['FEATURES'] = X_columns
                RESULTS[dimension]['train']['X_DATA'] = X_train_ensemble
                if self.Type != 'regression':
                    most_freq_item, most_freq_count = Utils.most_common(Y_train)
                    RESULTS[dimension]['train']['F1'] = f1_score(Y_train_predicted, Y_train, average='micro')
                    RESULTS[dimension]['train']['F1_null'] = f1_score(most_freq_item * np.ones_like(Y_train), Y_train, average='micro')

                if total_folds > 1:

                    RESULTS[dimension]['test']['TEXT_IDs'] = test_IDs
                    RESULTS[dimension]['test']['FOLD'] = (fold_k + 1) * np.ones(len(Y_test))
                    RESULTS[dimension]['test']['PREDICTED_Y'] = Y_predicted
                    RESULTS[dimension]['test']['REAL_Y'] = Y_test
                    RESULTS[dimension]['test']['R'] = (np.corrcoef(Y_predicted, Y_test))[0, 1]
                    RESULTS[dimension]['test']['spearmanR'] = spearmanr(Y_predicted, Y_test).correlation
                    RESULTS[dimension]['test']['R2'] = RESULTS[dimension]['test']['R'] ** 2
                    RESULTS[dimension]['test']['NULL_Y'] = np.mean(Y_train) + 0 * Y_test
                    RESULTS[dimension]['test']['MSE_null'] = mean_squared_error(Y_test, RESULTS[dimension]['test']['NULL_Y'])
                    RESULTS[dimension]['test']['MSE'] = mean_squared_error(Y_predicted, Y_test)
                    RESULTS[dimension]['test']['FEATURES'] = 'see TRAIN data'
                    RESULTS[dimension]['test']['FEATURE_SCALE_MULTIPLIER'] = 'see TRAIN data'
                    RESULTS[dimension]['test']['MODEL'] = 'see TRAIN data'
                    RESULTS[dimension]['test']['X_DATA'] = None

                    if self.Type != 'regression':
                        RESULTS[dimension]['test']['F1_null'] = f1_score(most_freq_item * np.ones_like(Y_test), Y_test, average='micro')
                        RESULTS[dimension]['test']['F1'] = f1_score(Y_predicted, Y_test, average='micro')

                    if self.verbose_level > 1:
                        print('\n--------------- RESULTS over fold %i for \'%s\' ---------------' % (fold_k + 1, dimension))
                        print(Utils.str_single_fold(RESULTS[dimension]))
                        print('--------------------------------------------------------------\n')

                Y_train_predicted_ensemble_previous[dimension] = Y_train_predicted_ensemble
                Y_test_predicted_ensemble_previous[dimension] = Y_predicted_ensemble

        return RESULTS

if __name__ == "__main__":
    __spec__ = "ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>)"

    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}

    params={}

    # params['reliability']={}
    # params['reliability']['TextTypes'] = ('LEMMA', 'POS',) #
    # params['reliability']['FeatureMethod'] = ('TFIDF_2', 'EMBEDDING', 'CUSTOM', 'TAGS') # might drop BOW
    # params['reliability']['FeatureSelection'] = ('regression','all',300,) # possibly 100-300
    # params['reliability']['Algorithm'] = ('XGBoost', {'n_estimators': 300,'max_depth': 2}) # 0.2-0.4
    # params['reliability']['pass1_features'] = 4000

    # params['sentiment']={}
    # params['sentiment']['TextTypes'] = 'NORMAL_SENTENCES' # might add NORMAL
    # params['sentiment']['FeatureMethod'] = 'sequence'
    # params['sentiment']['FeatureSelection'] = ('regression', 'all', 350,) # 300-800
    # params['sentiment']['Algorithm'] = ['TEST',]#
    # params['sentiment']['pass1_features'] = 4000
    # # ('ElasticNet', {'l1_ratio': list(np.linspace(0.20,0.7,20))}),)
    # #
    #params['infovalue']={}
    #params['infovalue']['TextTypes'] = ('LEMMA',)
    #params['infovalue']['FeatureMethod'] = []
    #params['infovalue']['FeatureSelection'] = ('regression', 'all', 200,) # possibly 100-300
    #params['infovalue']['Algorithm'] = ('SEQUENCE',{'algorithm':'FASTTEXT','max_sequence':3000,'embedding_dim':100,'epochs':40,'batch':32,'dropout':0.10,'pooling_type':'attention'})#('ElasticNet', {'l1_ratio': 0.40}) # 0.3-0.5
    #params['infovalue']['pass1_features'] = 2500
    # # ('ElasticNet', {'l1_ratio': list(np.linspace(0.20,0.7,20))}),)
    #
    # params['subjectivity']['TextTypes'] = ('NORMAL','LEMMA', 'POS',)
    # params['subjectivity']['FeatureMethod'] = ('BOW_2', 'TFIDF_2', 'EMBEDDING', 'CUSTOM', 'TAGS',)
    # params['subjectivity']['FeatureSelection'] = ('regression', 'all', 1000,)
    # params['subjectivity']['Algorithm'] = ('GradientBoosting', {'n_estimators': 400, 'max_depth': 2, 'learning_rate': 0.1, 'min_samples_leaf': 5}) # 300-500, 0.05-0.15
    # params['subjectivity']['pass1_features'] = 5000
    # # ('ElasticNet', {'l1_ratio': list(np.linspace(0.20,0.7,20))}),)
    #
    # params['textlogic'] = {}
    # params['textlogic']['TextTypes'] = ('NORMAL',)
    # params['textlogic']['FeatureMethod'] = ('CUSTOM', 'TAGS',)
    # params['textlogic']['FeatureSelection'] = ('regression', 'all', 300,)
    # params['textlogic']['Algorithm'] = ('SEQUENCE',{'algorithm':'simple_LSTM','epochs':20})#('GradientBoosting', {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1, 'min_samples_leaf': 5})
    # params['textlogic']['pass1_features'] = 2000
    # # # ('ElasticNet', {'l1_ratio': list(np.linspace(0.20,0.7,20))}),)
    # #
    #params['infovalue']={}
    #params['writestyle']
    #params['writestyle']['FeatureSelection'] = ('regression', 'all', 500,)
    #params['writestyle']['Algorithm'] = ('SEQUENCE',{'algorithm':'simple_CNN2LSTM','filtercount':40,'filtersize':3,'emb_initializer':'random','rnn_units':120,'epochs':20,'embedding_dim':50,'batch':36,'dropout':0.20,'max_seq_length':1000})#('GradientBoosting', {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1, 'min_samples_leaf': 5})
    #params['infovalue']['LearnParams'] = {'pass1_features':3000,'FeatureSelection':('regression', 'all', 150),'TextTypes':('LEMMA','POS'),'FeatureMethod': ('TFIDF_3', 'EMBEDDING_NORMAL', 'CUSTOM', 'TAGS'),'Algorithm':('ElasticNet',{'l1_ratio': 0.60})}
    #params['infovalue']['LearnParams'] = {'pass1_features': 3000, 'FeatureSelection': ('regression', 'all', 500), 'TextTypes': ('LEMMA', 'POS'), 'FeatureMethod': ('BOW_2','TFIDF_2', 'EMBEDDING_NORMAL', 'CUSTOM', 'TAGS'), 'Algorithm': ('XGBoost', {'n_estimators': 150, 'max_depth': 2})},
    #params['reliability']['LearnParams'] = {'FeatureScaler': 'StandardScaler','pass1_features': 3000, 'FeatureSelection': ('fisher', 'global', 400), 'TextTypes': ('LEMMA',), 'FeatureMethod': ('TFIDF_3','EMBEDDING_NORMAL', 'CUSTOM', 'TAGS'), 'Algorithm': ('ElasticNet_multitarget', {'l1_ratio': 0.50})}
    #params['infovalue']['LearnParams'] = {'pass1_features':3000,'FeatureSelection':('fisher', 'all', 80),'TextTypes':('LEMMA','NORMAL'),'FeatureMethod': ('BOW_2','TFIDF_2', 'EMBEDDING_LEMMA', 'CUSTOM', 'TAGS'),'Algorithm':('ElasticNet',{'l1_ratio': 0.20,'n_alphas': 100})},
    #params['infovalue']['LearnParams'] = {'pass1_features':3500,'FeatureSelection':('regression', 'all', 50),'TextTypes':('LEMMA','POS'),'FeatureMethod': ('BOW_2','TFIDF_2', 'EMBEDDING_LEMMA', 'CUSTOM', 'TAGS'),'Algorithm':('ElasticNet',{'l1_ratio': 0.40,'n_alphas': 100})} #'Algorithm':('StringKernel', {'nu':0.50,'ngram':'2to10'})})
    #params['infovalue']['LearnParams'] = {'FeatureScaler': 'StandardScaler','pass1_features':3000,'FeatureSelection':('fisher', 'all', 300),'TextTypes':('LEMMA','POS'),'FeatureMethod': ('TFIDF_3', 'EMBEDDING_NORMAL', 'CUSTOM', 'TAGS'),'Algorithm':['RandomForest_multitarget',{'n_estimators': 100,'max_features':'sqrt','max_depth': 4}]}

    #params['infovalue']['LearnParams'] = {'FeatureScaler': 'StandardScaler','pass1_features_SVD': None, 'pass1_features':5000, 'FeatureSelection': ('model','all',700), 'TextTypes': ('LEMMA',), 'FeatureMethod': ('BOW_3','TFIDF_3', 'EMBEDDING_NORMAL', 'CUSTOM', 'TAGS'),
    #                                        'Algorithm': ('ElasticNet', {'l1_ratio': 0.01})}

    #params['infovalue']['ensemble_mix']=(0.333,0.333,0.333)

    #params['writestyle']['pass1_features'] = 5000
     # # ('ElasticNet', {'l1_ratio': list(np.linspace(0.20,0.7,20))}),)
    #params['textlogic']=params['infovalue']
    #params['writestyle']=params['infovalue']params['reliability']

    LearnParams = {}
    LearnParams_RIDGE = (
        {'TextTypes': ['LEMMA'], 'Algorithm': ('Ridge', {'n_alphas': 25}), 'pass1_features': 200, 'FeatureSelection': ['fisher', 'global', 800], 'FeatureScaler': 'MaxAbsScaler', 'FeatureMethod': ['BOW_3', 'TFIDF_3', 'EMBEDDING_NORMAL', 'CUSTOM', 'TAGS'], 'pass1_features_SVD': None},
        {'TextTypes': ['LEMMA', 'NORMAL'], 'Algorithm': ('Ridge', {'n_alphas': 30}), 'FeatureSelection': ['regression', 'global', 1000], 'FeatureScaler': 'MaxAbsScaler', 'pass1_features': 200, 'pass1_features_SVD': None, 'FeatureMethod': ['TFIDF_3', 'EMBEDDING_NORMAL', 'CUSTOM', 'TAGS']}
        )
    LearnParams['reliability'] = LearnParams_RIDGE

    RootPath = r'D:/JanneK/Documents/git_repos/LaureaTextAnalyzer/'

    CV_folds=1

    my_object = MainPipeline(RootPath=RootPath,RECOMPUTE_ALL=0,RANDOM_TEST=0,SINGLE_THREAD_TEST=1,
                 CV_folds = CV_folds,
                 Type = 'regression',
                 verbose_level=1,
                 N_workers=5,
                 run_type='final',
                 is_sequential = True,
                 LearnParams=LearnParams
                 )
    result = my_object.run()

